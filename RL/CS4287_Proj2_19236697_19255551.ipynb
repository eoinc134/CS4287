{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19236697 Sean Ryan and 19255551 Eoin Costello\n",
    "# Code executes in full without error\n",
    "# Source we used for the base of writing the code, adapted as per our needs: https://www.youtube.com/watch?v=Uc6qBg7mM2Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[clasic_control] in c:\\users\\19255551\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.26.2)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gym 0.26.2 does not provide the extra 'clasic_control'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\19255551\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[clasic_control]) (2.2.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\19255551\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[clasic_control]) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\19255551\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[clasic_control]) (1.23.4)\n",
      "Requirement already satisfied: pygame in c:\\users\\19255551\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym[clasic_control]\n",
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setup Environment#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "esG2hlYaRoPW"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np \n",
    "\n",
    "import time \n",
    "import math \n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random seed initialisation#\n",
    "RANDOM_SEED = 5\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "NGVW6kr5R8U4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2)\n",
      "State space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "#Set up Environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Action Space: {}\".format(env.action_space))\n",
    "print(\"State space: {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Parameters#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters#\n",
    "learning_rate = 0.7\n",
    "discount_factor = 0.618\n",
    "\n",
    "epsilon = 1\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "decay = 0.01\n",
    "\n",
    "train_episodes = 300\n",
    "test_episodes = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setup Deep Q Network#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep RL Agent#\n",
    "def agent(state_shape, action_shape):\n",
    "    lr = 0.001 #initial learning rate\n",
    "    init = tf.keras.initializers.HeUniform(seed=RANDOM_SEED)\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate==lr), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Q values\n",
    "def get_qs(model, state, step):\n",
    "    return model.predict(state.reshape([1, state.shape[0]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train our RL Agent#\n",
    "#Q Update#\n",
    "def train(env, replay_memory, model, target_model, done):\n",
    "\n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "\n",
    "    batch_size = 64 * 2\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    current_states = np.array([transition[0] for transition in mini_batch])\n",
    "    current_qs_list = model.predict(current_states)\n",
    "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
    "    future_qs_list = target_model.predict(new_current_states)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "\n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "        \n",
    "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initialize Target and Main Models#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent(env.observation_space.shape, env.action_space.n)\n",
    "target_model = agent(env.observation_space.shape, env.action_space.n)\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=50_000)\n",
    "target_update_counter = 0\n",
    "steps_to_update_target_model = 0\n",
    "\n",
    "# X = states, y = actions\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 , Training Reward: 13.0, Final Reward: 1.0\n",
      "Episode: 1 , Training Reward: 18.0, Final Reward: 1.0\n",
      "Episode: 2 , Training Reward: 24.0, Final Reward: 1.0\n",
      "Episode: 3 , Training Reward: 22.0, Final Reward: 1.0\n",
      "Episode: 4 , Training Reward: 18.0, Final Reward: 1.0\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Episode: 5 , Training Reward: 22.0, Final Reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19255551\\AppData\\Local\\Temp\\ipykernel_26432\\479210719.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  encoded = np.asarray(observation)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     encoded \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(observation)\n\u001b[0;32m     19\u001b[0m     encoded_reshaped \u001b[39m=\u001b[39m encoded\u001b[39m.\u001b[39mreshape([\u001b[39m1\u001b[39m, encoded\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]])\n\u001b[1;32m---> 20\u001b[0m     predicted \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(encoded_reshaped)\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m     22\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(predicted)\n\u001b[0;32m     24\u001b[0m new_observation, reward, done, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\19255551\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\19255551\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "for episode in range(train_episodes):\n",
    "        total_training_rewards = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            steps_to_update_target_model += 1\n",
    "            if True:\n",
    "                env.render()\n",
    "\n",
    "            random_number = np.random.rand()\n",
    "            # Explore using the Epsilon Greedy Exploration Strategy\n",
    "            if random_number <= epsilon:\n",
    "                # Explore\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Exploit best known action\n",
    "                encoded = np.asarray(observation)\n",
    "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "                predicted = model.predict(encoded_reshaped).flatten()\n",
    "                \n",
    "                action = np.argmax(predicted)\n",
    "           \n",
    "            new_observation, reward, done, truncated, info = env.step(action)\n",
    "            replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "            # Update the Main Network using the Bellman Equation\n",
    "            if steps_to_update_target_model % 4 == 0 or done:\n",
    "                train(env, replay_memory, model, target_model, done)\n",
    "\n",
    "            observation = new_observation\n",
    "            total_training_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print('Episode: {} , Training Reward: {}, Final Reward: {}'.format(episode, total_training_rewards, reward))\n",
    "                total_training_rewards += 1\n",
    "\n",
    "                if steps_to_update_target_model >= 100:\n",
    "                    #print('Copying main network weights to the target network weights')\n",
    "                    target_model.set_weights(model.get_weights())\n",
    "                    steps_to_update_target_model = 0\n",
    "                break\n",
    "\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CartPole.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "13d68b6e597a2adaa719b896bbd110a86ce55e5129ba268a11dfafcdded08205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
